Namespace(annpath='train.json', bs=64, clip_len=16, cnnmethod='resnet', cuda=True, dataparallel=True, emb_init='../wordvectors/glove.840B.300d-char.txt', embedding_size=300, feature_size=512, framepath='frames', imsize=224, log_every=10, lr=8e-05, lstm_memory=512, lstm_pretrain_ep=100, lstm_stacks=1, max_epochs=1000, max_seqlen=200, meta_path='videometa_train.json', mode='train', model_path='../models', momentum=0.9, n_cpu=8, num_layers=10, patience=10, rnnmethod='LSTM', root_path='/ssd1/dsets/activitynet_captions', start_from_ep=0, token_level=True, vocabpath='vocab_char.json')
loaded dictionary from /ssd1/dsets/activitynet_captions/vocab_char.json
dictionary length: 91 words
succesfully initialized embeddings from ../wordvectors/glove.840B.300d-char.txt
using 4 gpus...
# of params in model : 16294379
start decoder pretraining, doing for 100 epochs
iter 000010/000141 | nll loss: 4.5402 | 1.3344s per loop
iter 000020/000141 | nll loss: 4.5377 | 0.3102s per loop
iter 000030/000141 | nll loss: 4.5381 | 0.3889s per loop
iter 000040/000141 | nll loss: 4.5359 | 0.4847s per loop
iter 000050/000141 | nll loss: 4.5317 | 0.5575s per loop
iter 000060/000141 | nll loss: 4.5313 | 0.4836s per loop
iter 000070/000141 | nll loss: 4.5286 | 0.4764s per loop
iter 000080/000141 | nll loss: 4.5259 | 0.4937s per loop
iter 000090/000141 | nll loss: 4.5258 | 0.5575s per loop
iter 000100/000141 | nll loss: 4.5221 | 0.4852s per loop
iter 000110/000141 | nll loss: 4.5213 | 0.4886s per loop
iter 000120/000141 | nll loss: 4.5216 | 0.4913s per loop
iter 000130/000141 | nll loss: 4.5157 | 0.5575s per loop
iter 000140/000141 | nll loss: 4.5144 | 0.4429s per loop
epoch 0001/0100 done (pretrain), loss: 4.514476
saved pretrained decoder model to ../models/LSTM_pre/b064_s224_l016/ep0001.ckpt
/home/seito/anaconda3/envs/anetchallenge/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
iter 000010/000141 | nll loss: 4.5142 | 0.9307s per loop
iter 000020/000141 | nll loss: 4.5110 | 0.4469s per loop
iter 000030/000141 | nll loss: 4.5085 | 0.4554s per loop
iter 000040/000141 | nll loss: 4.5079 | 0.4336s per loop
iter 000050/000141 | nll loss: 4.5058 | 0.6013s per loop
iter 000060/000141 | nll loss: 4.5039 | 0.4595s per loop
iter 000070/000141 | nll loss: 4.5017 | 0.4466s per loop
iter 000080/000141 | nll loss: 4.4992 | 0.4170s per loop
iter 000090/000141 | nll loss: 4.4990 | 0.6028s per loop
iter 000100/000141 | nll loss: 4.4976 | 0.4732s per loop
iter 000110/000141 | nll loss: 4.4928 | 0.4272s per loop
iter 000120/000141 | nll loss: 4.4932 | 0.4507s per loop
iter 000130/000141 | nll loss: 4.4903 | 0.5733s per loop
iter 000140/000141 | nll loss: 4.4873 | 0.4106s per loop
epoch 0002/0100 done (pretrain), loss: 4.488536
saved pretrained decoder model to ../models/LSTM_pre/b064_s224_l016/ep0002.ckpt
iter 000010/000141 | nll loss: 4.4885 | 0.9472s per loop
iter 000020/000141 | nll loss: 4.4844 | 0.4481s per loop
iter 000030/000141 | nll loss: 4.4809 | 0.4625s per loop
iter 000040/000141 | nll loss: 4.4815 | 0.4256s per loop
iter 000050/000141 | nll loss: 4.4771 | 0.5958s per loop
iter 000060/000141 | nll loss: 4.4772 | 0.4483s per loop
iter 000070/000141 | nll loss: 4.4771 | 0.4500s per loop
iter 000080/000141 | nll loss: 4.4718 | 0.4257s per loop
iter 000090/000141 | nll loss: 4.4719 | 0.5804s per loop
iter 000100/000141 | nll loss: 4.4702 | 0.4275s per loop
iter 000110/000141 | nll loss: 4.4687 | 0.4574s per loop
iter 000120/000141 | nll loss: 4.4676 | 0.4456s per loop
iter 000130/000141 | nll loss: 4.4676 | 0.5870s per loop
iter 000140/000141 | nll loss: 4.4621 | 0.4116s per loop
epoch 0003/0100 done (pretrain), loss: 4.460326
saved pretrained decoder model to ../models/LSTM_pre/b064_s224_l016/ep0003.ckpt
iter 000010/000141 | nll loss: 4.4599 | 0.9587s per loop
iter 000020/000141 | nll loss: 4.4575 | 0.4405s per loop
iter 000030/000141 | nll loss: 4.4592 | 0.4687s per loop
iter 000040/000141 | nll loss: 4.4525 | 0.4560s per loop
iter 000050/000141 | nll loss: 4.4535 | 0.6163s per loop
iter 000060/000141 | nll loss: 4.4523 | 0.4516s per loop
iter 000070/000141 | nll loss: 4.4489 | 0.4689s per loop
iter 000080/000141 | nll loss: 4.4462 | 0.5124s per loop
iter 000090/000141 | nll loss: 4.4450 | 0.5857s per loop
iter 000100/000141 | nll loss: 4.4387 | 0.4709s per loop
iter 000110/000141 | nll loss: 4.4410 | 0.4492s per loop
iter 000120/000141 | nll loss: 4.4383 | 0.4241s per loop
iter 000130/000141 | nll loss: 4.4353 | 0.6156s per loop
iter 000140/000141 | nll loss: 4.4388 | 0.4166s per loop
epoch 0004/0100 done (pretrain), loss: 4.435047
saved pretrained decoder model to ../models/LSTM_pre/b064_s224_l016/ep0004.ckpt
iter 000010/000141 | nll loss: 4.4347 | 0.9448s per loop
iter 000020/000141 | nll loss: 4.4313 | 0.4895s per loop
iter 000030/000141 | nll loss: 4.4312 | 0.5025s per loop
iter 000040/000141 | nll loss: 4.4286 | 0.4625s per loop
iter 000050/000141 | nll loss: 4.4297 | 0.5797s per loop
iter 000060/000141 | nll loss: 4.4246 | 0.4517s per loop
iter 000070/000141 | nll loss: 4.4196 | 0.4528s per loop
iter 000080/000141 | nll loss: 4.4217 | 0.4967s per loop
iter 000090/000141 | nll loss: 4.4172 | 0.5513s per loop
iter 000100/000141 | nll loss: 4.4159 | 0.4706s per loop
iter 000110/000141 | nll loss: 4.4143 | 0.4906s per loop
iter 000120/000141 | nll loss: 4.4126 | 0.4698s per loop
iter 000130/000141 | nll loss: 4.4107 | 0.6137s per loop
iter 000140/000141 | nll loss: 4.4086 | 0.4402s per loop
epoch 0005/0100 done (pretrain), loss: 4.408014
saved pretrained decoder model to ../models/LSTM_pre/b064_s224_l016/ep0005.ckpt
iter 000010/000141 | nll loss: 4.4060 | 1.0023s per loop
iter 000020/000141 | nll loss: 4.4063 | 0.4721s per loop
iter 000030/000141 | nll loss: 4.3973 | 0.4655s per loop
iter 000040/000141 | nll loss: 4.3962 | 0.4388s per loop
iter 000050/000141 | nll loss: 4.4027 | 0.6255s per loop
iter 000060/000141 | nll loss: 4.3962 | 0.4654s per loop
iter 000070/000141 | nll loss: 4.3915 | 0.4773s per loop
iter 000080/000141 | nll loss: 4.3907 | 0.4776s per loop
iter 000090/000141 | nll loss: 4.3891 | 0.6025s per loop
iter 000100/000141 | nll loss: 4.3883 | 0.4662s per loop
iter 000110/000141 | nll loss: 4.3890 | 0.4686s per loop
iter 000120/000141 | nll loss: 4.3876 | 0.4470s per loop
iter 000130/000141 | nll loss: 4.3824 | 0.5928s per loop
iter 000140/000141 | nll loss: 4.3816 | 0.4336s per loop
epoch 0006/0100 done (pretrain), loss: 4.381012
saved pretrained decoder model to ../models/LSTM_pre/b064_s224_l016/ep0006.ckpt
iter 000010/000141 | nll loss: 4.3801 | 1.0118s per loop
iter 000020/000141 | nll loss: 4.3741 | 0.4641s per loop
iter 000030/000141 | nll loss: 4.3754 | 0.4642s per loop
iter 000040/000141 | nll loss: 4.3694 | 0.4807s per loop
iter 000050/000141 | nll loss: 4.3652 | 0.6106s per loop
iter 000060/000141 | nll loss: 4.3656 | 0.4377s per loop
iter 000070/000141 | nll loss: 4.3665 | 0.4470s per loop
iter 000080/000141 | nll loss: 4.3701 | 0.4686s per loop
iter 000090/000141 | nll loss: 4.3582 | 0.6021s per loop
iter 000100/000141 | nll loss: 4.3530 | 0.4444s per loop
iter 000110/000141 | nll loss: 4.3715 | 0.4411s per loop
iter 000120/000141 | nll loss: 4.3590 | 0.4642s per loop
iter 000130/000141 | nll loss: 4.3539 | 0.5865s per loop
iter 000140/000141 | nll loss: 4.3527 | 0.4019s per loop
epoch 0007/0100 done (pretrain), loss: 4.359347
saved pretrained decoder model to ../models/LSTM_pre/b064_s224_l016/ep0007.ckpt
iter 000010/000141 | nll loss: 4.3551 | 0.9514s per loop
iter 000020/000141 | nll loss: 4.3472 | 0.4593s per loop
iter 000030/000141 | nll loss: 4.3486 | 0.4515s per loop
iter 000040/000141 | nll loss: 4.3517 | 0.4673s per loop
iter 000050/000141 | nll loss: 4.3445 | 0.5857s per loop
iter 000060/000141 | nll loss: 4.3427 | 0.4808s per loop
iter 000070/000141 | nll loss: 4.3384 | 0.4597s per loop
iter 000080/000141 | nll loss: 4.3410 | 0.4660s per loop
iter 000090/000141 | nll loss: 4.3345 | 0.5839s per loop
iter 000100/000141 | nll loss: 4.3260 | 0.4455s per loop
iter 000110/000141 | nll loss: 4.3338 | 0.4854s per loop
iter 000120/000141 | nll loss: 4.3281 | 0.5007s per loop
iter 000130/000141 | nll loss: 4.3320 | 0.5935s per loop
iter 000140/000141 | nll loss: 4.3339 | 0.4384s per loop
epoch 0008/0100 done (pretrain), loss: 4.323554
saved pretrained decoder model to ../models/LSTM_pre/b064_s224_l016/ep0008.ckpt
iter 000010/000141 | nll loss: 4.3185 | 0.9630s per loop
iter 000020/000141 | nll loss: 4.3162 | 0.5144s per loop
iter 000030/000141 | nll loss: 4.3174 | 0.4450s per loop
iter 000040/000141 | nll loss: 4.3073 | 0.4685s per loop
iter 000050/000141 | nll loss: 4.3184 | 0.6165s per loop
iter 000060/000141 | nll loss: 4.3201 | 0.4666s per loop
iter 000070/000141 | nll loss: 4.3086 | 0.4511s per loop
iter 000080/000141 | nll loss: 4.3093 | 0.5053s per loop
iter 000090/000141 | nll loss: 4.3082 | 0.6130s per loop
iter 000100/000141 | nll loss: 4.3137 | 0.4843s per loop
iter 000110/000141 | nll loss: 4.3056 | 0.4744s per loop
iter 000120/000141 | nll loss: 4.3022 | 0.4634s per loop
iter 000130/000141 | nll loss: 4.3028 | 0.5788s per loop
iter 000140/000141 | nll loss: 4.2969 | 0.4272s per loop
epoch 0009/0100 done (pretrain), loss: 4.293693
saved pretrained decoder model to ../models/LSTM_pre/b064_s224_l016/ep0009.ckpt
iter 000010/000141 | nll loss: 4.2992 | 0.9970s per loop
iter 000020/000141 | nll loss: 4.2942 | 0.4613s per loop
iter 000030/000141 | nll loss: 4.2981 | 0.4746s per loop
iter 000040/000141 | nll loss: 4.2862 | 0.4567s per loop
iter 000050/000141 | nll loss: 4.2779 | 0.6163s per loop
iter 000060/000141 | nll loss: 4.2893 | 0.4859s per loop
iter 000070/000141 | nll loss: 4.2831 | 0.4403s per loop
iter 000080/000141 | nll loss: 4.2864 | 0.4726s per loop
iter 000090/000141 | nll loss: 4.2773 | 0.6015s per loop
iter 000100/000141 | nll loss: 4.2675 | 0.4573s per loop
iter 000110/000141 | nll loss: 4.2645 | 0.4652s per loop
iter 000120/000141 | nll loss: 4.2730 | 0.4449s per loop
iter 000130/000141 | nll loss: 4.2671 | 0.5871s per loop
iter 000140/000141 | nll loss: 4.2688 | 0.4325s per loop
epoch 0010/0100 done (pretrain), loss: 4.274499
saved pretrained decoder model to ../models/LSTM_pre/b064_s224_l016/ep0010.ckpt
iter 000010/000141 | nll loss: 4.2693 | 1.0028s per loop
iter 000020/000141 | nll loss: 4.2580 | 0.4692s per loop
iter 000030/000141 | nll loss: 4.2516 | 0.4867s per loop
iter 000040/000141 | nll loss: 4.2522 | 0.4687s per loop
iter 000050/000141 | nll loss: 4.2568 | 0.6110s per loop
iter 000060/000141 | nll loss: 4.2525 | 0.4510s per loop
iter 000070/000141 | nll loss: 4.2552 | 0.4471s per loop
iter 000080/000141 | nll loss: 4.2455 | 0.4776s per loop
iter 000090/000141 | nll loss: 4.2396 | 0.6345s per loop
iter 000100/000141 | nll loss: 4.2468 | 0.4853s per loop
iter 000110/000141 | nll loss: 4.2458 | 0.4877s per loop
iter 000120/000141 | nll loss: 4.2418 | 0.4608s per loop
iter 000130/000141 | nll loss: 4.2406 | 0.6027s per loop
iter 000140/000141 | nll loss: 4.2432 | 0.3944s per loop
epoch 0011/0100 done (pretrain), loss: 4.235627
saved pretrained decoder model to ../models/LSTM_pre/b064_s224_l016/ep0011.ckpt
iter 000010/000141 | nll loss: 4.2322 | 0.9653s per loop
iter 000020/000141 | nll loss: 4.2308 | 0.4695s per loop
iter 000030/000141 | nll loss: 4.2305 | 0.4578s per loop
iter 000040/000141 | nll loss: 4.2191 | 0.4657s per loop
iter 000050/000141 | nll loss: 4.2262 | 0.6043s per loop
iter 000060/000141 | nll loss: 4.2249 | 0.4828s per loop
iter 000070/000141 | nll loss: 4.2205 | 0.4495s per loop
iter 000080/000141 | nll loss: 4.2344 | 0.4398s per loop
iter 000090/000141 | nll loss: 4.2241 | 0.6045s per loop
iter 000100/000141 | nll loss: 4.2098 | 0.4545s per loop
iter 000110/000141 | nll loss: 4.2070 | 0.4701s per loop
iter 000120/000141 | nll loss: 4.2151 | 0.4589s per loop
iter 000130/000141 | nll loss: 4.2026 | 0.5617s per loop
iter 000140/000141 | nll loss: 4.2122 | 0.4305s per loop
epoch 0012/0100 done (pretrain), loss: 4.219083
saved pretrained decoder model to ../models/LSTM_pre/b064_s224_l016/ep0012.ckpt
iter 000010/000141 | nll loss: 4.2050 | 0.9767s per loop
iter 000020/000141 | nll loss: 4.2044 | 0.4607s per loop
iter 000030/000141 | nll loss: 4.2056 | 0.4370s per loop
iter 000040/000141 | nll loss: 4.1944 | 0.5003s per loop
iter 000050/000141 | nll loss: 4.2045 | 0.6050s per loop
iter 000060/000141 | nll loss: 4.2013 | 0.4767s per loop
iter 000070/000141 | nll loss: 4.2030 | 0.4922s per loop
iter 000080/000141 | nll loss: 4.1863 | 0.4821s per loop
iter 000090/000141 | nll loss: 4.1856 | 0.6284s per loop
iter 000100/000141 | nll loss: 4.1716 | 0.4494s per loop
iter 000110/000141 | nll loss: 4.1856 | 0.4590s per loop
iter 000120/000141 | nll loss: 4.1882 | 0.4703s per loop
iter 000130/000141 | nll loss: 4.1869 | 0.5898s per loop
iter 000140/000141 | nll loss: 4.1752 | 0.4209s per loop
epoch 0013/0100 done (pretrain), loss: 4.169662
saved pretrained decoder model to ../models/LSTM_pre/b064_s224_l016/ep0013.ckpt
iter 000010/000141 | nll loss: 4.1857 | 0.9711s per loop
iter 000020/000141 | nll loss: 4.1711 | 0.4564s per loop
iter 000030/000141 | nll loss: 4.1734 | 0.4538s per loop
iter 000040/000141 | nll loss: 4.1859 | 0.4717s per loop
iter 000050/000141 | nll loss: 4.1648 | 0.5933s per loop
iter 000060/000141 | nll loss: 4.1720 | 0.4690s per loop
iter 000070/000141 | nll loss: 4.1553 | 0.5013s per loop
iter 000080/000141 | nll loss: 4.1722 | 0.4313s per loop
iter 000090/000141 | nll loss: 4.1412 | 0.6272s per loop
iter 000100/000141 | nll loss: 4.1519 | 0.4504s per loop
iter 000110/000141 | nll loss: 4.1697 | 0.4905s per loop
iter 000120/000141 | nll loss: 4.1389 | 0.4651s per loop
iter 000130/000141 | nll loss: 4.1463 | 0.6206s per loop
iter 000140/000141 | nll loss: 4.1537 | 0.4261s per loop
epoch 0014/0100 done (pretrain), loss: 4.152714
saved pretrained decoder model to ../models/LSTM_pre/b064_s224_l016/ep0014.ckpt
iter 000010/000141 | nll loss: 4.1372 | 1.0185s per loop
iter 000020/000141 | nll loss: 4.1387 | 0.4644s per loop
iter 000030/000141 | nll loss: 4.1364 | 0.4843s per loop
iter 000040/000141 | nll loss: 4.1394 | 0.4772s per loop
iter 000050/000141 | nll loss: 4.1414 | 0.5981s per loop
iter 000060/000141 | nll loss: 4.1436 | 0.4600s per loop
iter 000070/000141 | nll loss: 4.1258 | 0.4535s per loop
iter 000080/000141 | nll loss: 4.1260 | 0.5030s per loop
iter 000090/000141 | nll loss: 4.1311 | 0.5628s per loop
iter 000100/000141 | nll loss: 4.1222 | 0.4805s per loop
iter 000110/000141 | nll loss: 4.1211 | 0.4592s per loop
iter 000120/000141 | nll loss: 4.1345 | 0.4796s per loop
iter 000130/000141 | nll loss: 4.1340 | 0.6104s per loop
iter 000140/000141 | nll loss: 4.1120 | 0.4059s per loop
epoch 0015/0100 done (pretrain), loss: 4.119648
saved pretrained decoder model to ../models/LSTM_pre/b064_s224_l016/ep0015.ckpt
iter 000010/000141 | nll loss: 4.1028 | 1.0000s per loop
iter 000020/000141 | nll loss: 4.1162 | 0.4610s per loop
iter 000030/000141 | nll loss: 4.1138 | 0.4995s per loop
iter 000040/000141 | nll loss: 4.1050 | 0.4812s per loop
iter 000050/000141 | nll loss: 4.0966 | 0.5845s per loop
iter 000060/000141 | nll loss: 4.1112 | 0.4600s per loop
iter 000070/000141 | nll loss: 4.1078 | 0.4749s per loop
iter 000080/000141 | nll loss: 4.1118 | 0.4320s per loop
iter 000090/000141 | nll loss: 4.1172 | 0.6112s per loop
iter 000100/000141 | nll loss: 4.1073 | 0.4634s per loop
iter 000110/000141 | nll loss: 4.0915 | 0.4650s per loop
iter 000120/000141 | nll loss: 4.0840 | 0.4788s per loop
iter 000130/000141 | nll loss: 4.0907 | 0.5881s per loop
iter 000140/000141 | nll loss: 4.0953 | 0.4027s per loop
epoch 0016/0100 done (pretrain), loss: 4.090150
saved pretrained decoder model to ../models/LSTM_pre/b064_s224_l016/ep0016.ckpt
iter 000010/000141 | nll loss: 4.0759 | 0.9727s per loop
iter 000020/000141 | nll loss: 4.0834 | 0.4526s per loop
iter 000030/000141 | nll loss: 4.0679 | 0.4416s per loop
iter 000040/000141 | nll loss: 4.0862 | 0.4912s per loop
iter 000050/000141 | nll loss: 4.0849 | 0.6012s per loop
iter 000060/000141 | nll loss: 4.0927 | 0.4708s per loop
iter 000070/000141 | nll loss: 4.0562 | 0.5067s per loop
iter 000080/000141 | nll loss: 4.0834 | 0.4528s per loop
iter 000090/000141 | nll loss: 4.0642 | 0.6371s per loop
iter 000100/000141 | nll loss: 4.0735 | 0.4755s per loop
iter 000110/000141 | nll loss: 4.0930 | 0.4706s per loop
iter 000120/000141 | nll loss: 4.0537 | 0.4388s per loop
iter 000130/000141 | nll loss: 4.0694 | 0.6361s per loop
iter 000140/000141 | nll loss: 4.0808 | 0.4025s per loop
epoch 0017/0100 done (pretrain), loss: 4.059564
saved pretrained decoder model to ../models/LSTM_pre/b064_s224_l016/ep0017.ckpt
iter 000010/000141 | nll loss: 4.0545 | 0.9776s per loop
iter 000020/000141 | nll loss: 4.0715 | 0.4627s per loop
iter 000030/000141 | nll loss: 4.0628 | 0.4605s per loop
iter 000040/000141 | nll loss: 4.0614 | 0.4575s per loop
iter 000050/000141 | nll loss: 4.0745 | 0.6216s per loop
iter 000060/000141 | nll loss: 4.0474 | 0.4923s per loop
iter 000070/000141 | nll loss: 4.0482 | 0.4412s per loop
iter 000080/000141 | nll loss: 4.0475 | 0.4937s per loop
iter 000090/000141 | nll loss: 4.0554 | 0.6082s per loop
iter 000100/000141 | nll loss: 4.0480 | 0.4550s per loop
iter 000110/000141 | nll loss: 4.0415 | 0.4646s per loop
iter 000120/000141 | nll loss: 4.0591 | 0.4766s per loop
iter 000130/000141 | nll loss: 4.0353 | 0.6195s per loop
iter 000140/000141 | nll loss: 4.0348 | 0.4418s per loop
epoch 0018/0100 done (pretrain), loss: 4.040715
saved pretrained decoder model to ../models/LSTM_pre/b064_s224_l016/ep0018.ckpt
iter 000010/000141 | nll loss: 4.0436 | 0.9703s per loop
iter 000020/000141 | nll loss: 4.0288 | 0.4689s per loop
iter 000030/000141 | nll loss: 4.0311 | 0.4428s per loop
iter 000040/000141 | nll loss: 4.0497 | 0.4642s per loop
iter 000050/000141 | nll loss: 4.0217 | 0.5739s per loop
iter 000060/000141 | nll loss: 4.0395 | 0.5170s per loop
iter 000070/000141 | nll loss: 4.0282 | 0.4847s per loop
iter 000080/000141 | nll loss: 4.0245 | 0.4592s per loop
iter 000090/000141 | nll loss: 4.0359 | 0.5815s per loop
