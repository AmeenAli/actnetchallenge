Namespace(annpath='train.json', bs=32, clip_len=16, cuda=True, dataparallel=True, emb_init='../wordvectors/glove.840B.300d-char.txt', embedding_size=300, feature_size=512, framepath='frames', ft_begin_index=0, imsize=224, log_every=10, lr=0.0001, lstm_memory=512, lstm_pretrain_ep=10, lstm_stacks=1, manual_seed=1, max_epochs=1000, max_seqlen=30, meta_path='videometa_train.json', mode='train', model_path='../models', modeldepth=18, modelname='resnet', momentum=0.9, n_classes=400, n_cpu=8, n_finetune_classes=400, norm_value=1, num_layers=18, patience=10, pretrain_path='../models/resnet-18-kinetics.pth', resnet_shortcut='A', resnext_cardinality=32, rnnmethod='LSTM', root_path='/ssd1/dsets/activitynet_captions', sample_duration=16, sample_size=112, start_from_ep=0, token_level=False, vocabpath='vocab.json', wide_resnet_k=2)
loaded dictionary from /ssd1/dsets/activitynet_captions/vocab.json
dictionary length: 11181 words
loading pretrained model ../models/resnet-18-kinetics.pth
succesfully initialized embeddings from ../wordvectors/glove.840B.300d-char.txt
using 4 gpus...
# of params in model : 44146405
start decoder pretraining, doing for 10 epochs
iter 000010/000283 | nll loss: 9.3209 | 1.0423s per loop
iter 000020/000283 | nll loss: 9.3278 | 0.1672s per loop
iter 000030/000283 | nll loss: 9.3201 | 0.2473s per loop
iter 000040/000283 | nll loss: 9.3235 | 0.2336s per loop
iter 000050/000283 | nll loss: 9.3273 | 0.3800s per loop
iter 000060/000283 | nll loss: 9.3229 | 0.2435s per loop
iter 000070/000283 | nll loss: 9.3248 | 0.2465s per loop
iter 000080/000283 | nll loss: 9.3123 | 0.2450s per loop
iter 000090/000283 | nll loss: 9.3203 | 0.3731s per loop
iter 000100/000283 | nll loss: 9.3128 | 0.2661s per loop
iter 000110/000283 | nll loss: 9.3188 | 0.2445s per loop
iter 000120/000283 | nll loss: 9.3165 | 0.2267s per loop
iter 000130/000283 | nll loss: 9.3190 | 0.3696s per loop
iter 000140/000283 | nll loss: 9.3224 | 0.2588s per loop
iter 000150/000283 | nll loss: 9.3231 | 0.2427s per loop
iter 000160/000283 | nll loss: 9.3157 | 0.2314s per loop
iter 000170/000283 | nll loss: 9.3166 | 0.3545s per loop
iter 000180/000283 | nll loss: 9.3164 | 0.2801s per loop
iter 000190/000283 | nll loss: 9.3076 | 0.2357s per loop
iter 000200/000283 | nll loss: 9.3125 | 0.2448s per loop
iter 000210/000283 | nll loss: 9.3078 | 0.3305s per loop
iter 000220/000283 | nll loss: 9.3131 | 0.2955s per loop
iter 000230/000283 | nll loss: 9.3121 | 0.2265s per loop
iter 000240/000283 | nll loss: 9.3099 | 0.2358s per loop
iter 000250/000283 | nll loss: 9.3071 | 0.2842s per loop
iter 000260/000283 | nll loss: 9.3142 | 0.3145s per loop
iter 000270/000283 | nll loss: 9.3105 | 0.2196s per loop
iter 000280/000283 | nll loss: 9.3116 | 0.2411s per loop
epoch 0001/0010 done (pretrain), loss: 9.308195
saved pretrained decoder model to ../models/LSTM_pre/b032_s224_l016/ep0001.ckpt
/home/seito/anaconda3/envs/anetchallenge/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
iter 000010/000283 | nll loss: 9.3074 | 0.6993s per loop
iter 000020/000283 | nll loss: 9.3113 | 0.2476s per loop
iter 000030/000283 | nll loss: 9.3128 | 0.2440s per loop
iter 000040/000283 | nll loss: 9.3045 | 0.2411s per loop
iter 000050/000283 | nll loss: 9.3111 | 0.3964s per loop
iter 000060/000283 | nll loss: 9.3063 | 0.2454s per loop
iter 000070/000283 | nll loss: 9.3144 | 0.2348s per loop
iter 000080/000283 | nll loss: 9.3011 | 0.2266s per loop
iter 000090/000283 | nll loss: 9.2994 | 0.3737s per loop
iter 000100/000283 | nll loss: 9.3094 | 0.2300s per loop
iter 000110/000283 | nll loss: 9.3092 | 0.2205s per loop
iter 000120/000283 | nll loss: 9.3082 | 0.2215s per loop
iter 000130/000283 | nll loss: 9.3111 | 0.3695s per loop
iter 000140/000283 | nll loss: 9.3009 | 0.2347s per loop
iter 000150/000283 | nll loss: 9.3049 | 0.2402s per loop
iter 000160/000283 | nll loss: 9.3050 | 0.2283s per loop
iter 000170/000283 | nll loss: 9.3041 | 0.3937s per loop
iter 000180/000283 | nll loss: 9.2957 | 0.2325s per loop
iter 000190/000283 | nll loss: 9.2976 | 0.2315s per loop
iter 000200/000283 | nll loss: 9.2934 | 0.2379s per loop
iter 000210/000283 | nll loss: 9.3056 | 0.3825s per loop
iter 000220/000283 | nll loss: 9.3013 | 0.2401s per loop
iter 000230/000283 | nll loss: 9.2974 | 0.2387s per loop
iter 000240/000283 | nll loss: 9.3034 | 0.2499s per loop
iter 000250/000283 | nll loss: 9.2964 | 0.4010s per loop
iter 000260/000283 | nll loss: 9.2993 | 0.2357s per loop
iter 000270/000283 | nll loss: 9.2950 | 0.2446s per loop
iter 000280/000283 | nll loss: 9.2986 | 0.2405s per loop
epoch 0002/0010 done (pretrain), loss: 9.294234
saved pretrained decoder model to ../models/LSTM_pre/b032_s224_l016/ep0002.ckpt
iter 000010/000283 | nll loss: 9.2951 | 0.6832s per loop
iter 000020/000283 | nll loss: 9.3043 | 0.2357s per loop
iter 000030/000283 | nll loss: 9.2897 | 0.2339s per loop
iter 000040/000283 | nll loss: 9.2931 | 0.2379s per loop
iter 000050/000283 | nll loss: 9.3016 | 0.3877s per loop
iter 000060/000283 | nll loss: 9.2990 | 0.2480s per loop
iter 000070/000283 | nll loss: 9.3026 | 0.2473s per loop
iter 000080/000283 | nll loss: 9.2834 | 0.2411s per loop
iter 000090/000283 | nll loss: 9.2936 | 0.4083s per loop
iter 000100/000283 | nll loss: 9.2959 | 0.2368s per loop
iter 000110/000283 | nll loss: 9.2899 | 0.2482s per loop
iter 000120/000283 | nll loss: 9.2995 | 0.2522s per loop
iter 000130/000283 | nll loss: 9.2948 | 0.4079s per loop
iter 000140/000283 | nll loss: 9.2944 | 0.2399s per loop
iter 000150/000283 | nll loss: 9.2932 | 0.2404s per loop
iter 000160/000283 | nll loss: 9.2879 | 0.2250s per loop
iter 000170/000283 | nll loss: 9.2970 | 0.4082s per loop
iter 000180/000283 | nll loss: 9.2929 | 0.2319s per loop
iter 000190/000283 | nll loss: 9.2908 | 0.2416s per loop
iter 000200/000283 | nll loss: 9.2864 | 0.2728s per loop
iter 000210/000283 | nll loss: 9.2846 | 0.3892s per loop
iter 000220/000283 | nll loss: 9.2783 | 0.2349s per loop
iter 000230/000283 | nll loss: 9.2883 | 0.2686s per loop
iter 000240/000283 | nll loss: 9.2960 | 0.2293s per loop
iter 000250/000283 | nll loss: 9.2831 | 0.3759s per loop
iter 000260/000283 | nll loss: 9.2838 | 0.2282s per loop
iter 000270/000283 | nll loss: 9.2953 | 0.2226s per loop
iter 000280/000283 | nll loss: 9.2912 | 0.2277s per loop
epoch 0003/0010 done (pretrain), loss: 9.283721
saved pretrained decoder model to ../models/LSTM_pre/b032_s224_l016/ep0003.ckpt
iter 000010/000283 | nll loss: 9.2819 | 0.7265s per loop
iter 000020/000283 | nll loss: 9.2797 | 0.2237s per loop
iter 000030/000283 | nll loss: 9.2841 | 0.2254s per loop
iter 000040/000283 | nll loss: 9.2960 | 0.2389s per loop
iter 000050/000283 | nll loss: 9.2845 | 0.4053s per loop
iter 000060/000283 | nll loss: 9.2885 | 0.2579s per loop
iter 000070/000283 | nll loss: 9.2878 | 0.2546s per loop
iter 000080/000283 | nll loss: 9.2853 | 0.2592s per loop
iter 000090/000283 | nll loss: 9.2742 | 0.4332s per loop
iter 000100/000283 | nll loss: 9.2748 | 0.2551s per loop
iter 000110/000283 | nll loss: 9.2679 | 0.2253s per loop
iter 000120/000283 | nll loss: 9.2834 | 0.2462s per loop
iter 000130/000283 | nll loss: 9.2862 | 0.3793s per loop
iter 000140/000283 | nll loss: 9.2757 | 0.2261s per loop
iter 000150/000283 | nll loss: 9.2765 | 0.2440s per loop
iter 000160/000283 | nll loss: 9.2801 | 0.2473s per loop
iter 000170/000283 | nll loss: 9.2672 | 0.3965s per loop
iter 000180/000283 | nll loss: 9.2799 | 0.2694s per loop
iter 000190/000283 | nll loss: 9.2607 | 0.2427s per loop
iter 000200/000283 | nll loss: 9.2775 | 0.2466s per loop
iter 000210/000283 | nll loss: 9.2712 | 0.4108s per loop
iter 000220/000283 | nll loss: 9.2683 | 0.2634s per loop
iter 000230/000283 | nll loss: 9.2680 | 0.2460s per loop
iter 000240/000283 | nll loss: 9.2736 | 0.2533s per loop
iter 000250/000283 | nll loss: 9.2701 | 0.4329s per loop
iter 000260/000283 | nll loss: 9.2733 | 0.2598s per loop
iter 000270/000283 | nll loss: 9.2771 | 0.2402s per loop
iter 000280/000283 | nll loss: 9.2688 | 0.2400s per loop
epoch 0004/0010 done (pretrain), loss: 9.272068
saved pretrained decoder model to ../models/LSTM_pre/b032_s224_l016/ep0004.ckpt
iter 000010/000283 | nll loss: 9.2662 | 0.6862s per loop
iter 000020/000283 | nll loss: 9.2809 | 0.2218s per loop
iter 000030/000283 | nll loss: 9.2755 | 0.2450s per loop
iter 000040/000283 | nll loss: 9.2559 | 0.2412s per loop
iter 000050/000283 | nll loss: 9.2713 | 0.3942s per loop
iter 000060/000283 | nll loss: 9.2617 | 0.2479s per loop
iter 000070/000283 | nll loss: 9.2882 | 0.2495s per loop
iter 000080/000283 | nll loss: 9.2680 | 0.2403s per loop
iter 000090/000283 | nll loss: 9.2669 | 0.3821s per loop
iter 000100/000283 | nll loss: 9.2561 | 0.2393s per loop
iter 000110/000283 | nll loss: 9.2651 | 0.2422s per loop
iter 000120/000283 | nll loss: 9.2681 | 0.2517s per loop
iter 000130/000283 | nll loss: 9.2755 | 0.4027s per loop
iter 000140/000283 | nll loss: 9.2747 | 0.2570s per loop
iter 000150/000283 | nll loss: 9.2569 | 0.2725s per loop
iter 000160/000283 | nll loss: 9.2689 | 0.2813s per loop
iter 000170/000283 | nll loss: 9.2643 | 0.4221s per loop
iter 000180/000283 | nll loss: 9.2623 | 0.2673s per loop
iter 000190/000283 | nll loss: 9.2523 | 0.2645s per loop
iter 000200/000283 | nll loss: 9.2671 | 0.2606s per loop
iter 000210/000283 | nll loss: 9.2717 | 0.4165s per loop
iter 000220/000283 | nll loss: 9.2584 | 0.2672s per loop
iter 000230/000283 | nll loss: 9.2636 | 0.2471s per loop
iter 000240/000283 | nll loss: 9.2840 | 0.2422s per loop
iter 000250/000283 | nll loss: 9.2507 | 0.4222s per loop
iter 000260/000283 | nll loss: 9.2627 | 0.2548s per loop
iter 000270/000283 | nll loss: 9.2597 | 0.2701s per loop
iter 000280/000283 | nll loss: 9.2634 | 0.2319s per loop
epoch 0005/0010 done (pretrain), loss: 9.269696
saved pretrained decoder model to ../models/LSTM_pre/b032_s224_l016/ep0005.ckpt
iter 000010/000283 | nll loss: 9.2467 | 0.7086s per loop
iter 000020/000283 | nll loss: 9.2579 | 0.2428s per loop
iter 000030/000283 | nll loss: 9.2380 | 0.2614s per loop
iter 000040/000283 | nll loss: 9.2510 | 0.2674s per loop
iter 000050/000283 | nll loss: 9.2582 | 0.3874s per loop
iter 000060/000283 | nll loss: 9.2569 | 0.2475s per loop
iter 000070/000283 | nll loss: 9.2602 | 0.2476s per loop
iter 000080/000283 | nll loss: 9.2518 | 0.2486s per loop
iter 000090/000283 | nll loss: 9.2574 | 0.4129s per loop
iter 000100/000283 | nll loss: 9.2759 | 0.2418s per loop
iter 000110/000283 | nll loss: 9.2527 | 0.2623s per loop
iter 000120/000283 | nll loss: 9.2364 | 0.2867s per loop
iter 000130/000283 | nll loss: 9.2542 | 0.3970s per loop
iter 000140/000283 | nll loss: 9.2599 | 0.2429s per loop
iter 000150/000283 | nll loss: 9.2400 | 0.2514s per loop
iter 000160/000283 | nll loss: 9.2450 | 0.2675s per loop
iter 000170/000283 | nll loss: 9.2498 | 0.3973s per loop
iter 000180/000283 | nll loss: 9.2346 | 0.2539s per loop
iter 000190/000283 | nll loss: 9.2441 | 0.2526s per loop
iter 000200/000283 | nll loss: 9.2464 | 0.2871s per loop
iter 000210/000283 | nll loss: 9.2543 | 0.4456s per loop
iter 000220/000283 | nll loss: 9.2637 | 0.2589s per loop
iter 000230/000283 | nll loss: 9.2340 | 0.2608s per loop
iter 000240/000283 | nll loss: 9.2528 | 0.2594s per loop
iter 000250/000283 | nll loss: 9.2388 | 0.4081s per loop
iter 000260/000283 | nll loss: 9.2625 | 0.2442s per loop
iter 000270/000283 | nll loss: 9.2432 | 0.2467s per loop
iter 000280/000283 | nll loss: 9.2419 | 0.2259s per loop
epoch 0006/0010 done (pretrain), loss: 9.232501
saved pretrained decoder model to ../models/LSTM_pre/b032_s224_l016/ep0006.ckpt
iter 000010/000283 | nll loss: 9.2398 | 0.6975s per loop
iter 000020/000283 | nll loss: 9.2447 | 0.2549s per loop
iter 000030/000283 | nll loss: 9.2481 | 0.2664s per loop
iter 000040/000283 | nll loss: 9.2391 | 0.2436s per loop
iter 000050/000283 | nll loss: 9.2406 | 0.4154s per loop
iter 000060/000283 | nll loss: 9.2504 | 0.2437s per loop
iter 000070/000283 | nll loss: 9.2478 | 0.2864s per loop
iter 000080/000283 | nll loss: 9.2365 | 0.2559s per loop
iter 000090/000283 | nll loss: 9.2315 | 0.4066s per loop
iter 000100/000283 | nll loss: 9.2375 | 0.2697s per loop
iter 000110/000283 | nll loss: 9.2394 | 0.2688s per loop
iter 000120/000283 | nll loss: 9.2387 | 0.2538s per loop
iter 000130/000283 | nll loss: 9.2524 | 0.4141s per loop
iter 000140/000283 | nll loss: 9.2439 | 0.2465s per loop
iter 000150/000283 | nll loss: 9.2387 | 0.2836s per loop
iter 000160/000283 | nll loss: 9.2448 | 0.2750s per loop
iter 000170/000283 | nll loss: 9.2444 | 0.3568s per loop
iter 000180/000283 | nll loss: 9.2438 | 0.2360s per loop
iter 000190/000283 | nll loss: 9.2467 | 0.3156s per loop
iter 000200/000283 | nll loss: 9.2350 | 0.2407s per loop
iter 000210/000283 | nll loss: 9.2180 | 0.3640s per loop
iter 000220/000283 | nll loss: 9.2313 | 0.2711s per loop
iter 000230/000283 | nll loss: 9.2450 | 0.3096s per loop
iter 000240/000283 | nll loss: 9.2206 | 0.2795s per loop
iter 000250/000283 | nll loss: 9.2108 | 0.4023s per loop
iter 000260/000283 | nll loss: 9.2352 | 0.2940s per loop
iter 000270/000283 | nll loss: 9.2261 | 0.2937s per loop
iter 000280/000283 | nll loss: 9.2211 | 0.2415s per loop
epoch 0007/0010 done (pretrain), loss: 9.231691
saved pretrained decoder model to ../models/LSTM_pre/b032_s224_l016/ep0007.ckpt
iter 000010/000283 | nll loss: 9.2256 | 0.7019s per loop
iter 000020/000283 | nll loss: 9.2128 | 0.2372s per loop
iter 000030/000283 | nll loss: 9.2388 | 0.2811s per loop
iter 000040/000283 | nll loss: 9.2300 | 0.2588s per loop
iter 000050/000283 | nll loss: 9.2393 | 0.4479s per loop
iter 000060/000283 | nll loss: 9.2220 | 0.2744s per loop
iter 000070/000283 | nll loss: 9.2321 | 0.2850s per loop
iter 000080/000283 | nll loss: 9.2378 | 0.2715s per loop
iter 000090/000283 | nll loss: 9.2274 | 0.4238s per loop
iter 000100/000283 | nll loss: 9.2438 | 0.2685s per loop
iter 000110/000283 | nll loss: 9.2202 | 0.2575s per loop
iter 000120/000283 | nll loss: 9.2271 | 0.2423s per loop
iter 000130/000283 | nll loss: 9.2233 | 0.4414s per loop
iter 000140/000283 | nll loss: 9.1998 | 0.2770s per loop
iter 000150/000283 | nll loss: 9.2171 | 0.2695s per loop
iter 000160/000283 | nll loss: 9.2245 | 0.2494s per loop
iter 000170/000283 | nll loss: 9.2205 | 0.3923s per loop
iter 000180/000283 | nll loss: 9.2316 | 0.2448s per loop
iter 000190/000283 | nll loss: 9.2360 | 0.2457s per loop
iter 000200/000283 | nll loss: 9.2346 | 0.2580s per loop
iter 000210/000283 | nll loss: 9.2160 | 0.4384s per loop
iter 000220/000283 | nll loss: 9.2257 | 0.2681s per loop
iter 000230/000283 | nll loss: 9.2167 | 0.2683s per loop
iter 000240/000283 | nll loss: 9.2313 | 0.2716s per loop
iter 000250/000283 | nll loss: 9.2261 | 0.4064s per loop
iter 000260/000283 | nll loss: 9.2206 | 0.2683s per loop
iter 000270/000283 | nll loss: 9.2324 | 0.2659s per loop
iter 000280/000283 | nll loss: 9.2322 | 0.2558s per loop
epoch 0008/0010 done (pretrain), loss: 9.214016
saved pretrained decoder model to ../models/LSTM_pre/b032_s224_l016/ep0008.ckpt
iter 000010/000283 | nll loss: 9.2212 | 0.7241s per loop
iter 000020/000283 | nll loss: 9.2139 | 0.2579s per loop
iter 000030/000283 | nll loss: 9.2107 | 0.2675s per loop
iter 000040/000283 | nll loss: 9.2029 | 0.2789s per loop
iter 000050/000283 | nll loss: 9.2161 | 0.4293s per loop
iter 000060/000283 | nll loss: 9.2124 | 0.2843s per loop
iter 000070/000283 | nll loss: 9.2256 | 0.2812s per loop
iter 000080/000283 | nll loss: 9.2302 | 0.2588s per loop
iter 000090/000283 | nll loss: 9.2135 | 0.4304s per loop
iter 000100/000283 | nll loss: 9.2176 | 0.2793s per loop
iter 000110/000283 | nll loss: 9.2018 | 0.2605s per loop
iter 000120/000283 | nll loss: 9.2026 | 0.2711s per loop
iter 000130/000283 | nll loss: 9.2073 | 0.3956s per loop
iter 000140/000283 | nll loss: 9.2303 | 0.2495s per loop
iter 000150/000283 | nll loss: 9.2168 | 0.2598s per loop
iter 000160/000283 | nll loss: 9.2105 | 0.2655s per loop
iter 000170/000283 | nll loss: 9.2178 | 0.4112s per loop
iter 000180/000283 | nll loss: 9.2081 | 0.2720s per loop
iter 000190/000283 | nll loss: 9.1910 | 0.2564s per loop
iter 000200/000283 | nll loss: 9.1992 | 0.2639s per loop
iter 000210/000283 | nll loss: 9.2144 | 0.4059s per loop
iter 000220/000283 | nll loss: 9.2005 | 0.2658s per loop
iter 000230/000283 | nll loss: 9.2077 | 0.2885s per loop
iter 000240/000283 | nll loss: 9.2021 | 0.2917s per loop
iter 000250/000283 | nll loss: 9.1979 | 0.4312s per loop
iter 000260/000283 | nll loss: 9.2233 | 0.2827s per loop
iter 000270/000283 | nll loss: 9.2003 | 0.2769s per loop
iter 000280/000283 | nll loss: 9.2166 | 0.2388s per loop
epoch 0009/0010 done (pretrain), loss: 9.176909
saved pretrained decoder model to ../models/LSTM_pre/b032_s224_l016/ep0009.ckpt
iter 000010/000283 | nll loss: 9.2247 | 0.7275s per loop
iter 000020/000283 | nll loss: 9.2037 | 0.2753s per loop
iter 000030/000283 | nll loss: 9.1928 | 0.2797s per loop
iter 000040/000283 | nll loss: 9.1866 | 0.2693s per loop
iter 000050/000283 | nll loss: 9.2356 | 0.4513s per loop
iter 000060/000283 | nll loss: 9.1976 | 0.2823s per loop
iter 000070/000283 | nll loss: 9.1998 | 0.2736s per loop
iter 000080/000283 | nll loss: 9.2013 | 0.2629s per loop
iter 000090/000283 | nll loss: 9.2155 | 0.4307s per loop
iter 000100/000283 | nll loss: 9.1964 | 0.2740s per loop
iter 000110/000283 | nll loss: 9.2016 | 0.2695s per loop
iter 000120/000283 | nll loss: 9.1844 | 0.2780s per loop
iter 000130/000283 | nll loss: 9.1813 | 0.4632s per loop
iter 000140/000283 | nll loss: 9.2102 | 0.2843s per loop
iter 000150/000283 | nll loss: 9.2004 | 0.2724s per loop
iter 000160/000283 | nll loss: 9.1844 | 0.2749s per loop
iter 000170/000283 | nll loss: 9.1859 | 0.4314s per loop
iter 000180/000283 | nll loss: 9.2207 | 0.2512s per loop
iter 000190/000283 | nll loss: 9.1964 | 0.2751s per loop
iter 000200/000283 | nll loss: 9.1802 | 0.2852s per loop
iter 000210/000283 | nll loss: 9.1970 | 0.3987s per loop
iter 000220/000283 | nll loss: 9.1633 | 0.2790s per loop
iter 000230/000283 | nll loss: 9.1787 | 0.3533s per loop
iter 000240/000283 | nll loss: 9.1937 | 0.2846s per loop
iter 000250/000283 | nll loss: 9.2023 | 0.3735s per loop
iter 000260/000283 | nll loss: 9.1911 | 0.2701s per loop
iter 000270/000283 | nll loss: 9.1710 | 0.3523s per loop
iter 000280/000283 | nll loss: 9.2044 | 0.2395s per loop
epoch 0010/0010 done (pretrain), loss: 9.173986
saved pretrained decoder model to ../models/LSTM_pre/b032_s224_l016/ep0010.ckpt
start training
Traceback (most recent call last):
  File "train.py", line 187, in <module>
    feature = video_encoder(clip)
  File "/home/seito/anaconda3/envs/anetchallenge/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/seito/anaconda3/envs/anetchallenge/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 143, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/seito/anaconda3/envs/anetchallenge/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 153, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/seito/anaconda3/envs/anetchallenge/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 83, in parallel_apply
    raise output
  File "/home/seito/anaconda3/envs/anetchallenge/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 59, in _worker
    output = module(*input, **kwargs)
  File "/home/seito/anaconda3/envs/anetchallenge/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/seito/anaconda3/envs/anetchallenge/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 142, in forward
    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
  File "/home/seito/anaconda3/envs/anetchallenge/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 147, in replicate
    return replicate(module, device_ids)
  File "/home/seito/anaconda3/envs/anetchallenge/lib/python3.6/site-packages/torch/nn/parallel/replicate.py", line 13, in replicate
    param_copies = Broadcast.apply(devices, *params)
  File "/home/seito/anaconda3/envs/anetchallenge/lib/python3.6/site-packages/torch/nn/parallel/_functions.py", line 21, in forward
    outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)
  File "/home/seito/anaconda3/envs/anetchallenge/lib/python3.6/site-packages/torch/cuda/comm.py", line 40, in broadcast_coalesced
    return torch._C._broadcast_coalesced(tensors, devices, buffer_size)
RuntimeError: all tensors must be on devices[0]
